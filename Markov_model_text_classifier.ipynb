{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model - Text Classifier\n",
    "\n",
    "Starting from 2 sets of poems by 2 different authors: Edgar Allan Poe and Robert Frost, build a text classifier that can distinguished between the 2 authors.\n",
    " - Compute train and test accuracy\n",
    " - Check for class imbalance, compute F1-score if imbalanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of the code:\n",
    " - Loop through each file, save each line to a list (one line == one sample)\n",
    " - Save the labels too\n",
    " - train-test split\n",
    " - Create a mapping from unique word to unique int index\n",
    "    - loop through data and tokenize each line (string split is enough)\n",
    "    - Assign each unique word a unique index\n",
    "    - create a special index for unknown word (words that could be in test set but are not in training set)  \n",
    " - Convert each line of text into integer lists\n",
    " - Train a Markov model for each class (Edgar Allan Poe / Robert Frost)\n",
    " - Use smoothing (add-one smoothing)\n",
    " - Do we need A and pi or just Log(A) and Log(pi)?\n",
    " - We also need to compute the priors p(class = k) to know if we need to take it into account in Baye's rule.\n",
    " - Write a function to compute the posterior for each class, given an input\n",
    " - Take the argmax over the posteriors to get the predicted class\n",
    " - Make predictions for both train and test sets\n",
    " - Compute accuracy for train and test sets\n",
    " - Check for class imbalance\n",
    " - Check confusion matrix and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-30 22:24:42--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 26622 (26K) [text/plain]\n",
      "Saving to: ‘edgar_allan_poe.txt’\n",
      "\n",
      "edgar_allan_poe.txt 100%[===================>]  26.00K  --.-KB/s    in 0.009s  \n",
      "\n",
      "2023-04-30 22:24:43 (2.92 MB/s) - ‘edgar_allan_poe.txt’ saved [26622/26622]\n",
      "\n",
      "--2023-04-30 22:24:43--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8003::154, 2606:50c0:8001::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 56286 (55K) [text/plain]\n",
      "Saving to: ‘robert_frost.txt’\n",
      "\n",
      "robert_frost.txt    100%[===================>]  54.97K  --.-KB/s    in 0.01s   \n",
      "\n",
      "2023-04-30 22:24:43 (4.54 MB/s) - ‘robert_frost.txt’ saved [56286/56286]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get datasets\n",
    "#!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "#!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  '../datasets/poems/edgar_allan_poe.txt',\n",
    "  '../datasets/poems/robert_frost.txt',\n",
    "]\n",
    "# label: Edgar Allan Poe => 0, Robert Frost => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "for i, filepath in enumerate(input_files):\n",
    "    with open(filepath) as file:\n",
    "      for line in file:\n",
    "         inputs.append(line)\n",
    "         labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping from each word to a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=(lambda x: x.lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clement/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Xtrain = vectorizer.fit_transform(inputs_train)\n",
    "Xtest = vectorizer.transform(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the vocabulary dict from vectorizer\n",
    "word2idx = vectorizer.vocabulary_\n",
    "idx2word = {v: k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a special index for \"unknown\" word\n",
    "idx2word[len(word2idx)] = \"unk\"\n",
    "word2idx[\"unk\"] = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  3786\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "print(\"vocabulary size: \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Markov model A0 and pi0 for Edgar Allan Poe, and another model A1 and pi1 for Robert Frost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute pi0 first.  \n",
    "pi0 is a vector of size vocab_size, and each component i is the total count of word i divided by total count of all words.  \n",
    "So for all Xtrain[k,:] where k such as Y[k] == 0, pi0[i] = np.sum(Xtrain_k[:,i])/np.sum(Xtrain_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0 = np.zeros(vocab_size)\n",
    "pi1 = np.zeros(vocab_size)\n",
    "sum_Xtrain_0 = 0\n",
    "sum_Xtrain_1 = 0\n",
    "for j in range(Xtrain.shape[1]):\n",
    "  for i in range(Xtrain.shape[0]):\n",
    "    if Ytrain[i] == 0:\n",
    "        # compute pi0\n",
    "        pi0[j] += Xtrain[i,j]\n",
    "        sum_Xtrain_0 += Xtrain[i,j]\n",
    "    if Ytrain[i] == 1:\n",
    "        # compute pi1\n",
    "        pi1[j] += Xtrain[i,j]\n",
    "        sum_Xtrain_1 += Xtrain[i,j]\n",
    "pi0 = pi0 / sum_Xtrain_0\n",
    "pi1 = pi1 / sum_Xtrain_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word with the highest initial probability for Edgar Allan Poe is:\n",
    "idx2word[np.argmax(pi0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute A0 and A1 now.  \n",
    "With Add-One smoothing, Aij = (count(word i to word j) + 1) / (count(word i) + vocab_size)\n",
    "In order to compute count(word i to word j), we first need to transfrom our list of words into list of int using word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Of', 'all', 'to', 'whom', 'thine', 'absence', 'is', 'the', 'night-\\n']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2185, 187, 3270, 3630, 3179, 137, 1626, 3132, 2128]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: word2idx[x],inputs_train[0].lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train_index = []\n",
    "A0 = np.ones([vocab_size, vocab_size]) # we start at 1 for Add-One smoothing\n",
    "A1 = np.ones([vocab_size, vocab_size])\n",
    "for i in range(len(inputs_train)):\n",
    "  # tokenize then transform into list of int\n",
    "  inputs_train_index.append(list(map(lambda x: word2idx[x],inputs_train[i].lower().split(' '))))\n",
    "  for j in range(len(inputs_train_index[i])-1):\n",
    "    if Ytrain[i] == 0:\n",
    "      # compute A0\n",
    "      A0[inputs_train_index[i][j],inputs_train_index[i][j+1]]+=1\n",
    "    if Ytrain[i] == 1:\n",
    "      # compute A1\n",
    "      A1[inputs_train_index[i][j],inputs_train_index[i][j+1]]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still need to divide by count(word i) + vocab_size\n",
    "indices_label_0 = [i for i in range(len(Ytrain)) if Ytrain[i] == 0]\n",
    "indices_label_1 = [i for i in range(len(Ytrain)) if Ytrain[i] == 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "column index (3785) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[115], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(vocab_size):\n\u001b[1;32m      5\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices_label_0:\n\u001b[0;32m----> 6\u001b[0m     word_counts_0[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m Xtrain[i,j]\n\u001b[1;32m      7\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m indices_label_1:\n\u001b[1;32m      8\u001b[0m     word_counts_1[j] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m Xtrain[i,j]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/sparse/_index.py:33\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m---> 33\u001b[0m     row, col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_indices(key)\n\u001b[1;32m     34\u001b[0m     \u001b[39m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/scipy/sparse/_index.py:143\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    141\u001b[0m col \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(col)\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m col \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mN \u001b[39mor\u001b[39;00m col \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m N:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcolumn index (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) out of range\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m col)\n\u001b[1;32m    144\u001b[0m \u001b[39mif\u001b[39;00m col \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    145\u001b[0m     col \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N\n",
      "\u001b[0;31mIndexError\u001b[0m: column index (3785) out of range"
     ]
    }
   ],
   "source": [
    "word_counts_0 = np.ones(vocab_size)*vocab_size\n",
    "word_counts_1 = np.ones(vocab_size)*vocab_size\n",
    "\n",
    "for j in range(vocab_size):\n",
    "  for i in indices_label_0:\n",
    "    word_counts_0[j] += Xtrain[i,j]\n",
    "  for i in indices_label_1:\n",
    "    word_counts_1[j] += Xtrain[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
