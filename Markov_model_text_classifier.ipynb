{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Model - Text Classifier\n",
    "\n",
    "Starting from 2 sets of poems by 2 different authors: Edgar Allan Poe and Robert Frost, build a text classifier that can distinguished between the 2 authors.\n",
    " - Compute train and test accuracy\n",
    " - Check for class imbalance, compute F1-score if imbalanced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of the code:\n",
    " - Loop through each file, save each line to a list (one line == one sample)\n",
    " - Save the labels too\n",
    " - train-test split\n",
    " - Create a mapping from unique word to unique int index\n",
    "    - loop through data and tokenize each line (string split is enough)\n",
    "    - Assign each unique word a unique index\n",
    "    - create a special index for unknown word (words that could be in test set but are not in training set)  \n",
    " - Convert each line of text into integer lists\n",
    " - Train a Markov model for each class (Edgar Allan Poe / Robert Frost)\n",
    " - Use smoothing (add-one smoothing)\n",
    " - Do we need A and pi or just Log(A) and Log(pi)?\n",
    " - We also need to compute the priors p(class = k) to know if we need to take it into account in Baye's rule.\n",
    " - Write a function to compute the posterior for each class, given an input\n",
    " - Take the argmax over the posteriors to get the predicted class\n",
    " - Make predictions for both train and test sets\n",
    " - Compute accuracy for train and test sets\n",
    " - Check for class imbalance\n",
    " - Check confusion matrix and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get datasets\n",
    "#!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
    "#!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = [\n",
    "  '../datasets/poems/edgar_allan_poe.txt',\n",
    "  '../datasets/poems/robert_frost.txt',\n",
    "]\n",
    "# label: Edgar Allan Poe => 0, Robert Frost => 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "for i, filepath in enumerate(input_files):\n",
    "    with open(filepath) as file:\n",
    "      for line in file:\n",
    "         line = line.rstrip().lower() # remove trailing white space and convert to lower case\n",
    "         if line:\n",
    "          #remove punctuation -- try with or without, it's not a clear change on the test set f1-score\n",
    "          line = line.translate(str.maketrans('','',string.punctuation))\n",
    "          inputs.append(line)\n",
    "          labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(inputs, labels, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a mapping from each word to a unique index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "vectorizer = CountVectorizer(tokenizer=(lambda x: x.lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clement/anaconda3/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Xtrain = vectorizer.fit_transform(inputs_train)\n",
    "Xtest = vectorizer.transform(inputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the vocabulary dict from vectorizer\n",
    "word2idx = vectorizer.vocabulary_\n",
    "idx2word = {v: k for k, v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a special index for \"unknown\" word\n",
    "idx2word[len(word2idx)] = \"unk\"\n",
    "word2idx[\"unk\"] = len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  2548\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "print(\"vocabulary size: \", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Markov model A0 and pi0 for Edgar Allan Poe, and another model A1 and pi1 for Robert Frost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute pi0 first.  \n",
    "pi0 is a vector of size vocab_size, and each component i is the total count of word i divided by total count of all words.  \n",
    "So for all Xtrain[k,:] where k such as Y[k] == 0, pi0[i] = np.sum(Xtrain_k[:,i])/np.sum(Xtrain_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0 = np.ones(vocab_size)\n",
    "pi1 = np.ones(vocab_size)\n",
    "sum_Xtrain_0 = vocab_size # with Add-One smoothing\n",
    "sum_Xtrain_1 = vocab_size\n",
    "for j in range(Xtrain.shape[1]):\n",
    "  for i in range(Xtrain.shape[0]):\n",
    "    if Ytrain[i] == 0:\n",
    "        # compute pi0\n",
    "        pi0[j] += Xtrain[i,j]\n",
    "        sum_Xtrain_0 += Xtrain[i,j]\n",
    "    if Ytrain[i] == 1:\n",
    "        # compute pi1\n",
    "        pi1[j] += Xtrain[i,j]\n",
    "        sum_Xtrain_1 += Xtrain[i,j]\n",
    "pi0 = np.log(pi0) - np.log(sum_Xtrain_0)\n",
    "pi1 = np.log(pi1) - np.log(sum_Xtrain_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The word with the highest initial probability for Edgar Allan Poe is:\n",
    "idx2word[np.argmax(pi0)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute A0 and A1 now.  \n",
    "With Add-One smoothing, Aij = (count(word i to word j) + 1) / (count(word i) + vocab_size)\n",
    "In order to compute count(word i to word j), we first need to transfrom our list of words into list of int using word2idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mother', 'of', 'god', 'be', 'with', 'me', 'still']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_train[0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1348, 1429, 841, 141, 2480, 1283, 2020]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: word2idx[x],inputs_train[0].lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train_index = []\n",
    "A0 = np.ones([vocab_size, vocab_size]) # we start at 1 for Add-One smoothing\n",
    "A1 = np.ones([vocab_size, vocab_size])\n",
    "for i in range(len(inputs_train)):\n",
    "  # tokenize then transform into list of int\n",
    "  inputs_train_index.append(list(map(lambda x: word2idx[x],inputs_train[i].lower().split(' '))))\n",
    "  for j in range(len(inputs_train_index[i])-1):\n",
    "    if Ytrain[i] == 0:\n",
    "      # compute A0\n",
    "      A0[inputs_train_index[i][j],inputs_train_index[i][j+1]]+=1\n",
    "    if Ytrain[i] == 1:\n",
    "      # compute A1\n",
    "      A1[inputs_train_index[i][j],inputs_train_index[i][j+1]]+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we still need to divide by count(word i) + vocab_size\n",
    "indices_label_0 = [i for i in range(len(Ytrain)) if Ytrain[i] == 0]\n",
    "indices_label_1 = [i for i in range(len(Ytrain)) if Ytrain[i] == 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_0 = np.ones(vocab_size)*vocab_size\n",
    "word_counts_1 = np.ones(vocab_size)*vocab_size\n",
    "\n",
    "for j in range(vocab_size-1):\n",
    "  for i in indices_label_0:\n",
    "    word_counts_0[j] += Xtrain[i,j]\n",
    "  for i in indices_label_1:\n",
    "    word_counts_1[j] += Xtrain[i,j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2595. 2620. 2548. ... 2548. 2549. 2548.]\n",
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(word_counts_0)\n",
    "print(A0[0,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to divide A0[i,:] and A1[i,:] by word_counts0[i] and word_count1[i] respectively.  \n",
    "We can also store A0 and A1 as log(A0) and log(A1). Same for pi0 and pi1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size):\n",
    "    A0[i,:] = np.log(A0[i,:]) - np.log(word_counts_0[i])\n",
    "    A1[i,:] = np.log(A1[i,:]) - np.log(word_counts_1[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.8613418 , -7.8613418 , -7.8613418 , ..., -7.8613418 ,\n",
       "        -7.8613418 , -7.8613418 ],\n",
       "       [-7.8709296 , -7.8709296 , -7.8709296 , ..., -7.8709296 ,\n",
       "        -7.8709296 , -7.8709296 ],\n",
       "       [-7.84306402, -7.84306402, -7.84306402, ..., -7.84306402,\n",
       "        -7.84306402, -7.84306402],\n",
       "       ...,\n",
       "       [-7.84306402, -7.84306402, -7.84306402, ..., -7.84306402,\n",
       "        -7.84306402, -7.84306402],\n",
       "       [-7.8434564 , -7.8434564 , -7.8434564 , ..., -7.8434564 ,\n",
       "        -7.8434564 , -7.8434564 ],\n",
       "       [-7.84306402, -7.84306402, -7.84306402, ..., -7.84306402,\n",
       "        -7.84306402, -7.84306402]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute P(k = 0) and P(k =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pobability of having a poem from Edgar Allan Poe in the training set:  0.3331269349845202  Probablity of Robert Frost:  0.6668730650154798\n",
      "-1.0992316754949722 -0.40515555850036844\n"
     ]
    }
   ],
   "source": [
    "Pk1 = np.sum(Ytrain) / len(Ytrain)\n",
    "Pk0 = 1-Pk1\n",
    "print(\"Pobability of having a poem from Edgar Allan Poe in the training set: \", Pk0, \" Probablity of Robert Frost: \", Pk1)\n",
    "logPk0 = np.log(Pk0)\n",
    "logPk1 = np.log(Pk1)\n",
    "print(logPk0, logPk1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build the classifier now!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we call x a poem, and k the class author with k = 0 for Edgar Allan Poe and k = 1 for Robert Frost.  \n",
    "We are looking for the estimator k* = argmax_k( p(class = k ) | x )  i.e the maximum of the probabilities of authors given and poem.  \n",
    "\n",
    "Bayes' Rule says:\n",
    "p(author | poem) = p(poem | author) * p(author) / p(poem)  \n",
    "\n",
    "Since we want the argmax, we can simplify the expression above by removing the denominator p(poem). Indeed p(poem) is a constant in our dataset and does not depend on author, so it can be removed. So now we are looking for:\n",
    "\n",
    "k* = argmax( p(author | poem) ) = argmax( p(poem | author) * p(author) )\n",
    "\n",
    "We can take the log of the expression above, as the log function is monotonously increasing, it does not change the order of the argmax.  \n",
    "\n",
    "k* = argmax_k( log(p(poem | author = k)) + log(p(author = k)) ) = argmax_k( log(p(x | k)) + log(p(k)) )  \n",
    "\n",
    "We just calculated p(k) above. And each Markov model gives us p(x | k).  \n",
    "\n",
    "Indeed, each poem is a list of words. We use our Markov models A0, pi0 and A1, pi1 to compute the probabilty of this sequence of word for each author. The highest probability is our estimate. That is p( x | k ).  \n",
    "\n",
    "So for a poem x of size N:  \n",
    "\n",
    "log(p( x | k = 0)) = log(pi0[x[0]]) + sum_on_i_to_N( log(A0[x[i],x[i+1]])) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_classifier(x):\n",
    "    # compute p(x | k = 0) and p(x | k=1)\n",
    "    logPx_k0 = pi0[x[0]]\n",
    "    logPx_k1 = pi1[x[0]]\n",
    "    for i in range(len(x)-1):\n",
    "        logPx_k0 += A0[x[i],x[i+1]]\n",
    "        logPx_k1 += A1[x[i],x[i+1]]\n",
    "    # add log( p(k = author) ) to get the full log(p(k=author | x))\n",
    "    log_p0_given_x = logPx_k0 + logPk0\n",
    "    log_p1_given_x = logPx_k1 + logPk1\n",
    "    return 0 if log_p0_given_x > log_p1_given_x else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(text_classifier(inputs_train_index[2])) # Ytrain[2] == 0\n",
    "print(text_classifier(inputs_train_index[3])) # Ytrain[3] == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert input_test into a list of word indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_words_to_indexes(word):\n",
    "  try:\n",
    "    return word2idx[word]\n",
    "  except Exception:\n",
    "    return word2idx['unk']\n",
    "\n",
    "inputs_test_index = []\n",
    "for i in range(len(inputs_test)):\n",
    "  inputs_test_index.append(list(map(lambda x: map_words_to_indexes(x), inputs_test[i].lower().split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_inputs_classifier(X):\n",
    "    Y = np.zeros(len(X))\n",
    "    for i in range(len(X)):\n",
    "        Y[i] = text_classifier(X[i])\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_pred = text_inputs_classifier(inputs_train_index)\n",
    "Y_test_pred = text_inputs_classifier(inputs_test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.9956656346749226\n",
      "test accuracy:  0.8200371057513914\n",
      "training f1-score:  0.996760758907913\n",
      "test f1-score:  0.8792029887920298\n",
      "training confusion matrix:  [[ 531    7]\n",
      " [   0 1077]]\n",
      "test confusion matrix:  [[ 89  91]\n",
      " [  6 353]]\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = accuracy_score(Ytrain, Y_train_pred)\n",
    "test_accuracy = accuracy_score(Ytest, Y_test_pred)\n",
    "training_f1_score = f1_score(Ytrain, Y_train_pred)\n",
    "test_f1_score = f1_score(Ytest, Y_test_pred)\n",
    "cm_train = confusion_matrix(Ytrain, Y_train_pred)\n",
    "cm_test = confusion_matrix(Ytest, Y_test_pred)\n",
    "\n",
    "print(\"training accuracy: \", training_accuracy)\n",
    "print(\"test accuracy: \", test_accuracy)\n",
    "print(\"training f1-score: \", training_f1_score)\n",
    "print(\"test f1-score: \", test_f1_score)\n",
    "print(\"training confusion matrix: \", cm_train)\n",
    "print(\"test confusion matrix: \", cm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
