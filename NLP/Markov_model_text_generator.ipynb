{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robert Frost poem generator using 2nd order Markov models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the curse of dimentionality, we are going to store our Markov matrixes as dictionaries. Indeed, storing them as Tensors would be inefficient as most of the components would be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = {} # start of a phrase, equivalent to pi0\n",
    "first_order = {} # equivalent to A0\n",
    "second_order = {} # equivalent to A[i,j,k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return s.translate(str.maketrans('','',string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "##!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to handle adding a key value pair to a dictionary. The values will be lists of words\n",
    "def add2dict(dictionary, key, value):\n",
    "  if key not in dictionary:\n",
    "    dictionary[key] = []\n",
    "  dictionary[key].append(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in open('../datasets/poems/robert_frost.txt'):\n",
    "    tokens = remove_punctuation(line.rstrip().lower()).split()\n",
    "    #tokens = line.rstrip().lower().split()\n",
    "    T = len(tokens)\n",
    "    for i in range(T):\n",
    "        t = tokens[i]\n",
    "        if i == 0: # here we are looking at the first word of the sentence\n",
    "            # mesure distribution of first word, fill in initial\n",
    "            initial[t] = initial.get(t,0.) + 1\n",
    "        else:\n",
    "            t_1 = tokens[i-1]\n",
    "            if i == T - 1: # here we are looking at the last word of the sentence\n",
    "                # measure probability of ending the line, create a fake 'END' token at the end of lines.\n",
    "                add2dict(second_order, (t_1, t), 'END')\n",
    "            if i == 1: # here we are looking at the 2nd word of the sentence\n",
    "                # measure distribution of second word given only first word\n",
    "                add2dict(first_order, t_1, t)\n",
    "            else:\n",
    "                t_2 = tokens[i-2]\n",
    "                add2dict(second_order, (t_2, t_1), t)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the distribution\n",
    "initial_total = sum(initial.values())\n",
    "for token, count in initial.items():\n",
    "    initial[token] = count / initial_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to convert our list of tokens into dictionaries of or {token: probability}\n",
    "\n",
    "def list2ProbabilityDict(ts):\n",
    "    # turn each list of tokens into a dictionary of probabilities\n",
    "    d = {}\n",
    "    n = len(ts)\n",
    "    for t in ts:\n",
    "        d[t] = d.get(t, 0.) + 1\n",
    "    for t, c in d.items():\n",
    "        d[t] = c / n\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's transform our dictionaries\n",
    "\n",
    "for t_1, ts in first_order.items():\n",
    "    # replace list with dict of proba\n",
    "    first_order[t_1] = list2ProbabilityDict(ts)\n",
    "\n",
    "for k, ts in second_order.items():\n",
    "    second_order[k] = list2ProbabilityDict(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we should create a function to sample words based on the dictionaries of probabilities we just calculated\n",
    "\n",
    "def sample_word(d):\n",
    "    # get a number between 0 and 1 from a uniform distribution\n",
    "    p0 = np.random.random()\n",
    "    # probability counter that will be incremented with each  probability of token in the keys of the probability dict\n",
    "    cumulative = 0 \n",
    "    for token, probability in d.items():\n",
    "        # at this point of the loop, we should have a probability of \"cumulative\" to select the token\n",
    "        cumulative += probability\n",
    "        # if the random number we have generated at the beginning is lower than the probability of current token, then return token\n",
    "        if p0 < cumulative:\n",
    "            return token\n",
    "    assert(False) # we should never get to this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    for i in range(4): # generate 4 lines\n",
    "      sentence = []\n",
    "\n",
    "      # sample initial word from initial dict\n",
    "      w0 = sample_word(initial)\n",
    "      sentence.append(w0)\n",
    "\n",
    "      # sample second word from first_order dict based on w0\n",
    "      w1 = sample_word(first_order[w0])\n",
    "      sentence.append(w1)\n",
    "\n",
    "      # second-order words until we find the 'END' token\n",
    "      while True:\n",
    "         w2 = sample_word(second_order[(w0, w1)])\n",
    "         if w2 == 'END':\n",
    "            break\n",
    "         sentence.append(w2)\n",
    "         w0 = w1\n",
    "         w1 = w2\n",
    "      print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then he came at me with one leg and a trapper looking in at the door and headboard of mothers bed is pushed\n",
      "and fell back from him on the world will end in fire\n",
      "from the house as far as i say\n",
      "beyond which god is\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
