{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-Of-Speech (POS) tagging using RNN (LSTM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to train a LSTM based model to tag classify words of sentences into categories of POS (Noun, Verb, Adjective, Determinant etc...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's first load and preprocess our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "# nltk.download('universal_tagset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for sentence_tag_pairs in corpus:\n",
    "  tokens = []\n",
    "  target = []\n",
    "  for token, tag in sentence_tag_pairs:\n",
    "    tokens.append(token)\n",
    "    target.append(tag)\n",
    "  inputs.append(tokens)\n",
    "  targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inputs[0])\n",
    "print(targets[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's create our training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, Embedding\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs, test_inputs, train_targets, test_targets = train_test_split(\n",
    "    inputs,\n",
    "    targets,\n",
    "    test_size=0.3,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize dataset\n",
    "\n",
    "Here we want to keep capitalized words because Nouns and Verbs could be similar when lowercase (Bill and to bill).\n",
    "We also need to add an 'UNK' token to map all unknown word to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to sequences\n",
    "\n",
    "MAX_VOCAB_SIZE = None\n",
    "\n",
    "# capitalization might be useful - test it\n",
    "should_lowercase = False\n",
    "word_tokenizer = Tokenizer(\n",
    "    num_words=MAX_VOCAB_SIZE,\n",
    "    lower=should_lowercase,\n",
    "    oov_token='UNK',\n",
    ")\n",
    "# otherwise unknown tokens will be removed and len(input) != len(target)\n",
    "# input words and target words will not be aligned!\n",
    "\n",
    "word_tokenizer.fit_on_texts(train_inputs)\n",
    "train_inputs_int = word_tokenizer.texts_to_sequences(train_inputs)\n",
    "test_inputs_int = word_tokenizer.texts_to_sequences(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get word -> integer mapping\n",
    "word2idx = word_tokenizer.word_index\n",
    "V = len(word2idx)\n",
    "print('Found %s unique tokens.' % V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(list_of_lists):\n",
    "  flattened = [val for sublist in list_of_lists for val in sublist]\n",
    "  return flattened\n",
    "\n",
    "all_train_targets = set(flatten(train_targets))\n",
    "all_test_targets = set(flatten(test_targets))\n",
    "# Make sure that our training set and test set have the same possible target values\n",
    "all_train_targets == all_test_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert targets to sequences\n",
    "tag_tokenizer = Tokenizer()\n",
    "tag_tokenizer.fit_on_texts(train_targets)\n",
    "train_targets_int = tag_tokenizer.texts_to_sequences(train_targets)\n",
    "test_targets_int = tag_tokenizer.texts_to_sequences(test_targets)\n",
    "\n",
    "# save for later\n",
    "train_targets_int_unpadded = train_targets_int\n",
    "test_targets_int_unpadded = test_targets_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before padding, find max document length\n",
    "# because we don't want to truncate any inputs\n",
    "# which would also truncate targets\n",
    "maxlen_train = max(len(sentence) for sentence in train_inputs)\n",
    "maxlen_test = max(len(sentence) for sentence in test_inputs)\n",
    "T = max((maxlen_train, maxlen_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences so that we get a N x T matrix\n",
    "train_inputs_int = pad_sequences(train_inputs_int, maxlen=T)\n",
    "print('Shape of data train tensor:', train_inputs_int.shape)\n",
    "\n",
    "test_inputs_int = pad_sequences(test_inputs_int, maxlen=T)\n",
    "print('Shape of data test tensor:', test_inputs_int.shape)\n",
    "\n",
    "train_targets_int = pad_sequences(train_targets_int, maxlen=T)\n",
    "print('Shape of train targets tensor:', train_targets_int.shape)\n",
    "\n",
    "test_targets_int = pad_sequences(test_targets_int, maxlen=T)\n",
    "print('Shape of test targets tensor:', test_targets_int.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes\n",
    "K = len(tag_tokenizer.word_index) + 1\n",
    "K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We get to choose embedding dimensionality\n",
    "D = 32\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Input(shape=(T,)),\n",
    "    Embedding(V + 1, D, mask_zero=True),\n",
    "    Bidirectional(LSTM(16, return_sequences=True)),\n",
    "    Dense(K)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit\n",
    "model.compile(\n",
    "  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# NOTE: you \"could\" speed this up by creating a custom loss, since Tensorflow's\n",
    "# implementation of mask_zero currently sucks, but it's sufficiently advanced\n",
    "# to be outside the scope of this course\n",
    "# In my experiments, CPU is faster than GPU in all cases, and CPU for custom\n",
    "# loss is faster than CPU for mask_zero\n",
    "\n",
    "# > 300-400s per epoch on CPU\n",
    "# > 30 MINUTES per epoch on GPU\n",
    "print('Training model...')\n",
    "r = model.fit(\n",
    "  train_inputs_int,\n",
    "  train_targets_int,\n",
    "  epochs=5,\n",
    "  validation_data=(test_inputs_int, test_targets_int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss per iteration\n",
    "plt.plot(r.history['loss'], label='train loss')\n",
    "plt.plot(r.history['val_loss'], label='val loss')\n",
    "plt.legend();\n",
    "\n",
    "# Plot accuracy per iteration\n",
    "plt.plot(r.history['accuracy'], label='train acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val acc')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get True model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first get length of each sequence\n",
    "train_lengths = []\n",
    "for sentence in train_inputs:\n",
    "  train_lengths.append(len(sentence))\n",
    "\n",
    "test_lengths = []\n",
    "for sentence in test_inputs:\n",
    "  test_lengths.append(len(sentence))\n",
    "\n",
    "\n",
    "train_probs = model.predict(train_inputs_int) # N x T x K\n",
    "train_predictions = []\n",
    "for probs, length in zip(train_probs, train_lengths):\n",
    "  # probs is T x K\n",
    "  probs_ = probs[-length:]\n",
    "  preds = np.argmax(probs_, axis=1)\n",
    "  train_predictions.append(preds)\n",
    "\n",
    "# flatten\n",
    "flat_train_predictions = flatten(train_predictions)\n",
    "flat_train_targets = flatten(train_targets_int_unpadded)\n",
    "\n",
    "test_probs = model.predict(test_inputs_int) # N x T x K\n",
    "test_predictions = []\n",
    "for probs, length in zip(test_probs, test_lengths):\n",
    "  # probs is T x K\n",
    "  probs_ = probs[-length:]\n",
    "  preds = np.argmax(probs_, axis=1)\n",
    "  test_predictions.append(preds)\n",
    "\n",
    "# flatten\n",
    "flat_test_predictions = flatten(test_predictions)\n",
    "flat_test_targets = flatten(test_targets_int_unpadded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"Train acc:\", accuracy_score(flat_train_targets, flat_train_predictions))\n",
    "print(\"Test acc:\", accuracy_score(flat_test_targets, flat_test_predictions))\n",
    "\n",
    "print(\"Train f1:\",\n",
    "      f1_score(flat_train_targets, flat_train_predictions, average='macro'))\n",
    "print(\"Test f1:\",\n",
    "      f1_score(flat_test_targets, flat_test_predictions, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
