{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer, pipeline\n",
    "from torchinfo import summary\n",
    "import json\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the glue dataset specialized in sentiment analysis sst2\n",
    "raw_datasets = load_dataset('glue', 'sst2') \n",
    "\n",
    "# try with \"amazon_polarity\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets\n",
    "# get a list of the methods available on a dataset\n",
    "# dir(raw_datasets['train'])\n",
    "# get the path of the documentation\n",
    "# type(raw_datasets['train'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can index the data set with a range of indices, just like a list.\n",
    "\n",
    "We can also get the name of the classes Ids, with the features method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': ['goes to absurd lengths ', \"for those moviegoers who complain that ` they do n't make movies like they used to anymore \", \"the part where nothing 's happening , \"], 'label': [0, 0, 0], 'idx': [10, 11, 12]}\n",
      "{'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][10:13])\n",
    "print(raw_datasets['train'].features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = 'bert-base-uncased'\n",
    "checkpoint = 'distilbert-base-uncased' # trains faster\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      " 'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102],\n",
      "               [101,\n",
      "                3397,\n",
      "                2053,\n",
      "                15966,\n",
      "                1010,\n",
      "                2069,\n",
      "                4450,\n",
      "                2098,\n",
      "                18201,\n",
      "                2015,\n",
      "                102],\n",
      "               [101,\n",
      "                2008,\n",
      "                7459,\n",
      "                2049,\n",
      "                3494,\n",
      "                1998,\n",
      "                10639,\n",
      "                2015,\n",
      "                2242,\n",
      "                2738,\n",
      "                3376,\n",
      "                2055,\n",
      "                2529,\n",
      "                3267,\n",
      "                102]]}\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = tokenizer(raw_datasets['train'][0:3]['sentence'], truncation=True)\n",
    "from pprint import pprint\n",
    "pprint(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(batch):\n",
    "  return tokenizer(batch['sentence'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 67349/67349 [00:00<00:00, 69985.77 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  'sentiment_analysis_trainer',\n",
    "  evaluation_strategy='epoch',\n",
    "  save_strategy='epoch',\n",
    "  num_train_epochs=1,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.safetensors: 100%|██████████| 268M/268M [00:40<00:00, 6.66MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with torchinfo summary\n",
    "summary(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the un-trained parameters in a list so that we can compare after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_before = []\n",
    "for name, p in model.named_parameters():\n",
    "  params_before.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cv/rf4bgk9d1sqgbjbk5ydbt4740000gn/T/ipykernel_51427/2092214033.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', 'sst2')\n",
      "Downloading builder script: 5.76kB [00:00, 10.5MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "metric = load_metric('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue_metric = evaluate.load('glue', 'sst2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.6666666666666666}\n",
      "{'accuracy': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "print(metric.compute(predictions=[1, 0, 1], references=[1, 0, 0]))\n",
    "print(glue_metric.compute(predictions=[1, 0, 1], references=[1, 0, 0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to create this function to match the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "  logits, labels = logits_and_labels\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  return glue_metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8419 [00:00<?, ?it/s]You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  6%|▌         | 501/8419 [01:04<13:25,  9.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4331, 'learning_rate': 4.7030526190759e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1002/8419 [01:53<11:26, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3376, 'learning_rate': 4.4061052381518e-05, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1501/8419 [02:42<10:47, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3399, 'learning_rate': 4.109157857227699e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2001/8419 [03:30<09:57, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3169, 'learning_rate': 3.812210476303599e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2501/8419 [04:17<09:30, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2854, 'learning_rate': 3.515263095379499e-05, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3002/8419 [05:05<08:33, 10.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.287, 'learning_rate': 3.218315714455399e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 3501/8419 [05:53<07:55, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2614, 'learning_rate': 2.9213683335312986e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4002/8419 [06:41<06:56, 10.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2566, 'learning_rate': 2.6244209526071984e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 4501/8419 [07:28<06:05, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2253, 'learning_rate': 2.3274735716830978e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 5001/8419 [08:15<05:29, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2427, 'learning_rate': 2.0305261907589976e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 5501/8419 [09:03<04:28, 10.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24, 'learning_rate': 1.7335788098348973e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 6001/8419 [09:50<03:53, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2152, 'learning_rate': 1.4366314289107971e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 6501/8419 [10:37<03:00, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2189, 'learning_rate': 1.1396840479866969e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 7002/8419 [12:25<02:16, 10.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2222, 'learning_rate': 8.427366670625965e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 7501/8419 [13:13<01:28, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.205, 'learning_rate': 5.457892861384962e-06, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 8001/8419 [14:01<00:40, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2141, 'learning_rate': 2.4884190521439603e-06, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 8419/8419 [14:47<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35671108961105347, 'eval_accuracy': 0.9048165137614679, 'eval_runtime': 4.4444, 'eval_samples_per_second': 196.203, 'eval_steps_per_second': 24.525, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8419/8419 [14:49<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 889.0206, 'train_samples_per_second': 75.756, 'train_steps_per_second': 9.47, 'train_loss': 0.2665088724702055, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.2665088724702055, metrics={'train_runtime': 889.0206, 'train_samples_per_second': 75.756, 'train_steps_per_second': 9.47, 'train_loss': 0.2665088724702055, 'epoch': 1.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('fine_tuned_sentiment_analysis_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = pipeline('text-classification', model='fine_tuned_sentiment_analysis_model', device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9988316893577576}]\n",
      "[{'label': 'LABEL_0', 'score': 0.9944121241569519}]\n"
     ]
    }
   ],
   "source": [
    "print(newmodel('This movie is great!'))\n",
    "print(newmodel('This movie is definitely not the best one I have seen lately!'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the config.json file of the model so that we give names to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = 'fine_tuned_sentiment_analysis_model/config.json'\n",
    "with open(config_path) as f:\n",
    "  j = json.load(f)\n",
    "\n",
    "j['id2label'] = {0: 'negative', 1: 'positive'}\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "  json.dump(j, f, indent=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel = pipeline('text-classification', model='fine_tuned_sentiment_analysis_model', device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9988316893577576}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel('This movie is great!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the model's parameters before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_after = []\n",
    "for name, p in model.named_parameters():\n",
    "  params_after.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13300.65\n",
      "89.88836\n",
      "1.7955394\n",
      "1.1239388\n",
      "1297.5863\n",
      "1.7593204\n",
      "1288.6768\n",
      "0.003140492\n",
      "1196.1934\n",
      "1.0895162\n",
      "1132.2045\n",
      "0.84817284\n",
      "1.6469915\n",
      "0.8338374\n",
      "4953.178\n",
      "5.767476\n",
      "4541.567\n",
      "0.69504136\n",
      "1.5693657\n",
      "0.7349802\n",
      "1282.287\n",
      "1.421695\n",
      "1277.6827\n",
      "0.0033160353\n",
      "1129.1239\n",
      "0.8821959\n",
      "1077.76\n",
      "0.7430595\n",
      "1.524674\n",
      "0.73176754\n",
      "4924.571\n",
      "5.45126\n",
      "4496.398\n",
      "0.7148503\n",
      "1.5088928\n",
      "0.80420995\n",
      "1275.1122\n",
      "1.504866\n",
      "1299.787\n",
      "0.0026822262\n",
      "1142.9791\n",
      "0.8354318\n",
      "1119.2639\n",
      "0.71764046\n",
      "1.5220361\n",
      "0.77335036\n",
      "4983.0083\n",
      "5.5937324\n",
      "4477.8286\n",
      "0.74410963\n",
      "1.3862755\n",
      "0.79971766\n",
      "1310.8688\n",
      "1.5094472\n",
      "1323.6318\n",
      "0.003470317\n",
      "1178.9724\n",
      "0.8257409\n",
      "1130.1505\n",
      "0.8058703\n",
      "1.4013195\n",
      "0.78627086\n",
      "4879.233\n",
      "5.4010105\n",
      "4265.6304\n",
      "0.7887438\n",
      "1.3308091\n",
      "0.8252969\n",
      "1200.4796\n",
      "1.6016853\n",
      "1191.1804\n",
      "0.002376105\n",
      "998.4962\n",
      "0.78034025\n",
      "1006.103\n",
      "0.90565497\n",
      "1.4091291\n",
      "0.94643724\n",
      "4442.6836\n",
      "5.0295005\n",
      "3520.7502\n",
      "0.7151392\n",
      "1.3262099\n",
      "0.7596569\n",
      "1107.5911\n",
      "1.1820103\n",
      "1130.9719\n",
      "0.001162499\n",
      "920.14825\n",
      "0.88509226\n",
      "900.5255\n",
      "1.1747687\n",
      "1.3468661\n",
      "1.1817567\n",
      "3555.3335\n",
      "4.628983\n",
      "3045.2712\n",
      "0.8681102\n",
      "1.2758484\n",
      "0.67757934\n",
      "863.9521\n",
      "0.9409889\n",
      "2.0969746\n",
      "0.0013929621\n"
     ]
    }
   ],
   "source": [
    "# verify that the params have changed\n",
    "for p1, p2 in zip(params_before, params_after):\n",
    "  print(np.sum(np.abs(p1 - p2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
